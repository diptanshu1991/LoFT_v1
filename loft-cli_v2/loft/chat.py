# loft/chat.py

import subprocess
import os

def run_chat(model_path, prompt, n_tokens=128):
    llama_cli = os.path.expanduser("/Users/diptanshukumar/llama.cpp/build/bin/llama-cli")

    if not os.path.isfile(llama_cli):
        print(f"âŒ llama-cli binary not found at: {llama_cli}")
        print("ğŸ‘‰ Please build llama.cpp first using cmake + make.")
        return

    if not os.path.isfile(model_path):
        print(f"âŒ Model file not found: {model_path}")
        return

    print(f"ğŸ§  Running inference on: {model_path}")
    print(f"ğŸ“¨ Prompt: {prompt}")

    command = [
        llama_cli,
        "-m", model_path,
        "-p", prompt,
        "-n", str(n_tokens)
    ]

    try:
        subprocess.run(command, check=True)
    except subprocess.CalledProcessError as e:
        print("âŒ Inference failed.")
        print(e)
